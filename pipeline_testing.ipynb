{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from warnings import simplefilter\n",
    "from variable_importance.dgp import DataGenerator\n",
    "from variable_importance.variable_importance_scoring import importance_score, cross_validation_scores\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "class FeatureSelector(SelectFromModel):\n",
    "    def __init__(self, estimator, threshold=None, prefit=False, norm_order=1, max_features=None, importance_getter='auto'):\n",
    "        super().__init__(estimator, threshold=threshold, prefit=prefit, norm_order=norm_order, max_features=max_features, importance_getter=importance_getter)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        super().fit(X=X, y=y, **fit_params)\n",
    "        self.feature_names = X.columns\n",
    "        self.feature_importances_ = self.get_support()\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, X, y, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        transformed_X = super().transform(X)\n",
    "        if transformed_X.shape[1] == 0:\n",
    "            if isinstance(X, np.ndarray):\n",
    "                return X[:, [0]]\n",
    "            elif hasattr(X, 'iloc'):  # Handling pandas DataFrame\n",
    "                return X.iloc[:, [0]]\n",
    "        return transformed_X\n",
    "    \n",
    "    def get_selected_features(self, feature_names=None):\n",
    "        selected_features = self.get_support()\n",
    "        if feature_names is None:\n",
    "            feature_names = self.feature_names\n",
    "        return [feature_names[i] for i, selected in enumerate(selected_features) if selected]\n",
    "\n",
    "class VI_Pipeline(Pipeline):\n",
    "    def __init__(self, steps, prediction_step=True, memory=None, verbose=False, vi_step=\"prediction\", vi_attr=\"feature_importances_\"):\n",
    "        super().__init__(steps, memory=memory, verbose=verbose)\n",
    "        self.selection_steps = steps[:-1] if prediction_step else steps[:]\n",
    "        self.prediction_step = prediction_step\n",
    "        self.vi_step = vi_step\n",
    "        self.vi_attr = vi_attr\n",
    "        self.feature_importances_ = None\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        self.features = X.columns\n",
    "        super().fit(X, y, **fit_params)\n",
    "        \n",
    "        self.feature_importances_ = self.recover_features(X.columns)\n",
    "        return self\n",
    "        \n",
    "    def fit_transform(self, X, y, **fit_params):\n",
    "        self.fit(X, y, fit_params=fit_params)\n",
    "        return self.transform(X)\n",
    "    \n",
    "    def recover_features(self, all_features=None, selector_name=\"feature_trimming\"):\n",
    "        all_features = all_features if all_features is not None else self.features\n",
    "        \n",
    "        feature_selector = self.named_steps[selector_name]\n",
    "        support_mask = feature_selector.get_support()\n",
    "        \n",
    "        full_importances = np.zeros(len(all_features))\n",
    "\n",
    "        if support_mask.any():\n",
    "            model_importances = getattr(self.named_steps[self.vi_step], self.vi_attr)\n",
    "            full_importances[support_mask] = model_importances\n",
    "\n",
    "        return full_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import fastsparsegams\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class FastSparseSklearn(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, max_support_size=10, tol=1e-8, lambda_0=0.025, gamma=0):\n",
    "        # self.data = data \n",
    "        # self.labels = labels\n",
    "        # self.data = data.to_numpy() if not isinstance(data, np.ndarray) else data\n",
    "        # self.labels = labels.to_numpy() if not isinstance(labels, np.ndarray) else labels\n",
    "        # self.data = self.transform(data)\n",
    "        # self.num_features = np.shape(data)[1]\n",
    "        self.max_support_size = max_support_size\n",
    "        # self.labels = self.labels[0].T\n",
    "        self.tol = tol\n",
    "        self.lambda_0 = lambda_0\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def transform(self, data):\n",
    "        # Check if data is a DataFrame and convert it directly to a numpy array\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            # Convert DataFrame to numpy array, ignoring indices and headers\n",
    "            data = data.values\n",
    "        elif not isinstance(data, np.ndarray):\n",
    "            # In case the input is neither DataFrame nor ndarray, convert it to ndarray\n",
    "            data = np.array(data)\n",
    "\n",
    "        # Ensure the data is of type float\n",
    "        data = data.astype(float)\n",
    "        return data\n",
    "    \n",
    "    def fit(self, data, labels):\n",
    "        data = self.transform(data)\n",
    "        labels = self.transform(labels)\n",
    "        self.model = fastsparsegams.fit(data, labels, penalty=\"L0\", max_support_size=self.max_support_size, algorithm = \"CDPSI\")\n",
    "        \n",
    "        coefficients = self.model.coeff(lambda_0=self.lambda_0, gamma=self.gamma).toarray()\n",
    "        self.coef_ = np.squeeze(coefficients) #might need to do more processing later\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = self.transform(X)\n",
    "        return self.model.predict(X, lambda_0=self.lambda_0, gamma=self.gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"feature_trimming__estimator__max_support_size\": [5, 10, 15, 20, 25],\n",
    "    \"feature_trimming__estimator__tol\": [1e-9, 1e-8, 1e-7, 1e-6],\n",
    "    \"feature_trimming__estimator__lambda_0\": [0.001, 0.005, 0.01, 0.05, 0.1],\n",
    "    #'feature_trimming__estimator__alpha': [0.01, 0.1, 1.0],  # Lasso alpha parameter\n",
    "    'prediction__learning_rate': [0.05, 0.1, 0.2],  # XGBoost learning rate\n",
    "    'prediction__n_estimators': [100, 200, 300],  # Number of trees in XGBoost\n",
    "    'prediction__max_depth': [3, 5, 7],  # Maximum depth of each tree in XGBoost\n",
    "}\n",
    "\n",
    "lasso = Lasso()\n",
    "fastsparse = FastSparseSklearn(10)\n",
    "xgboost = XGBRegressor()\n",
    "\n",
    "lasso_selector = FeatureSelector(lasso)\n",
    "fastsparse_selector = FeatureSelector(fastsparse, importance_getter=\"auto\")\n",
    "pipeline = VI_Pipeline(steps=[\n",
    "    ('feature_trimming', fastsparse_selector),\n",
    "    ('prediction', xgboost)\n",
    "], prediction_step=True, vi_step=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 1; dimension is 10 but corresponding boolean dimension is 11",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 13\u001b[0m\n\u001b[1;32m      8\u001b[0m y \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     11\u001b[0m fastsparse_selector\u001b[38;5;241m.\u001b[39mfit(X, y)\n\u001b[0;32m---> 13\u001b[0m reduced \u001b[38;5;241m=\u001b[39m \u001b[43mfastsparse_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 157\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    160\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    162\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    163\u001b[0m         )\n",
      "Cell \u001b[0;32mIn[93], line 33\u001b[0m, in \u001b[0;36mFeatureSelector.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m---> 33\u001b[0m     transformed_X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m transformed_X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, np\u001b[38;5;241m.\u001b[39mndarray):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 157\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    160\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    162\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    163\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/feature_selection/_base.py:96\u001b[0m, in \u001b[0;36mSelectorMixin.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# note: we use _safe_tags instead of _get_tags because this is a\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# public Mixin.\u001b[39;00m\n\u001b[1;32m     88\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m     89\u001b[0m     X,\n\u001b[1;32m     90\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m     reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     95\u001b[0m )\n\u001b[0;32m---> 96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/feature_selection/_base.py:112\u001b[0m, in \u001b[0;36mSelectorMixin._transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m X\u001b[38;5;241m.\u001b[39miloc[:, :\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mreshape((X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/__init__.py:355\u001b[0m, in \u001b[0;36m_safe_indexing\u001b[0;34m(X, indices, axis)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pandas_indexing(X, indices, indices_dtype, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_array_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _list_indexing(X, indices, indices_dtype)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/__init__.py:184\u001b[0m, in \u001b[0;36m_array_indexing\u001b[0;34m(array, key, key_dtype, axis)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    183\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m--> 184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m array[key] \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43marray\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 1; dimension is 10 but corresponding boolean dimension is 11"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection._base import _get_feature_importances\n",
    "from sklearn.feature_selection._from_model import _calculate_threshold\n",
    "\n",
    "dgp = DataGenerator(num_cols=10, num_rows=20, num_important=3, num_interaction_terms=0, effects='linear', noise_scale=0.5)\n",
    "dataset = dgp.generate_data()\n",
    "\n",
    "X = dataset.drop([\"target\"], axis=1)\n",
    "y = dataset[\"target\"]\n",
    "\n",
    "\n",
    "fastsparse_selector.fit(X, y)\n",
    "fastsparse_selector.estimator_.\n",
    "#reduced = fastsparse_selector.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "UserWarning",
     "evalue": "something bad happened",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/coding_projects/Data-Science-Competition/variable_importance/variable_importance_scoring.py:41\u001b[0m, in \u001b[0;36mcross_validation_scores\u001b[0;34m(cv, X, y, test_size, importance_attr, true_importances, score_function, verbose)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 41\u001b[0m     \u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/model_selection/_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 898\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/model_selection/_search.py:1809\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1808\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1809\u001b[0m \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1811\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[1;32m   1812\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1813\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/model_selection/_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    870\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[1;32m    873\u001b[0m     )\n\u001b[0;32m--> 875\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:414\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    408\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    409\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    410\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    411\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    412\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    413\u001b[0m     )\n\u001b[0;32m--> 414\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: \nAll the 30 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n30 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/home/kh/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/tmp/ipykernel_121877/3566350687.py\", line 58, in fit\n  File \"/home/kh/.local/lib/python3.8/site-packages/sklearn/base.py\", line 1152, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/home/kh/.local/lib/python3.8/site-packages/sklearn/pipeline.py\", line 423, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"/home/kh/.local/lib/python3.8/site-packages/sklearn/pipeline.py\", line 377, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"/home/kh/.local/lib/python3.8/site-packages/joblib/memory.py\", line 353, in __call__\n    return self.func(*args, **kwargs)\n  File \"/home/kh/.local/lib/python3.8/site-packages/sklearn/pipeline.py\", line 957, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"/home/kh/.local/lib/python3.8/site-packages/sklearn/utils/_set_output.py\", line 157, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"/tmp/ipykernel_121877/3566350687.py\", line 30, in fit_transform\n  File \"/home/kh/.local/lib/python3.8/site-packages/sklearn/utils/_set_output.py\", line 157, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"/tmp/ipykernel_121877/3566350687.py\", line 33, in transform\n  File \"/home/kh/.local/lib/python3.8/site-packages/sklearn/utils/_set_output.py\", line 157, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"/home/kh/.local/lib/python3.8/site-packages/sklearn/feature_selection/_base.py\", line 96, in transform\n    return self._transform(X)\n  File \"/home/kh/.local/lib/python3.8/site-packages/sklearn/feature_selection/_base.py\", line 112, in _transform\n    return _safe_indexing(X, mask, axis=1)\n  File \"/home/kh/.local/lib/python3.8/site-packages/sklearn/utils/__init__.py\", line 355, in _safe_indexing\n    return _array_indexing(X, indices, indices_dtype, axis=axis)\n  File \"/home/kh/.local/lib/python3.8/site-packages/sklearn/utils/__init__.py\", line 184, in _array_indexing\n    return array[key] if axis == 0 else array[:, key]\nIndexError: boolean index did not match indexed array along dimension 1; dimension is 10 but corresponding boolean dimension is 11\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUserWarning\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m rscv \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(pipeline, param_grid, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr2\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Now you can use this pipeline in your cross-validation function\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mcross_validation_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrscv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimportance_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfeature_importances_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_importances\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdgp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimportances\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m best_model \u001b[38;5;241m=\u001b[39m rscv\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Calculate predictions for the training set and the test set\u001b[39;00m\n",
      "File \u001b[0;32m~/coding_projects/Data-Science-Competition/variable_importance/variable_importance_scoring.py:43\u001b[0m, in \u001b[0;36mcross_validation_scores\u001b[0;34m(cv, X, y, test_size, importance_attr, true_importances, score_function, verbose)\u001b[0m\n\u001b[1;32m     41\u001b[0m     cv\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 43\u001b[0m     \u001b[43mwarnings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msomething bad happened\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;167;43;01mUserWarning\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     scores[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     scores[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mUserWarning\u001b[0m: something bad happened"
     ]
    }
   ],
   "source": [
    "dgp = DataGenerator(num_cols=10, num_rows=20, num_important=3, num_interaction_terms=0, effects='linear', noise_scale=0.5)\n",
    "dataset = dgp.generate_data()\n",
    "\n",
    "X = dataset.drop([\"target\"], axis=1)\n",
    "y = dataset[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "rscv = RandomizedSearchCV(pipeline, param_grid, scoring='r2', verbose=0, cv=3, n_iter=10, n_jobs=2)\n",
    "# Now you can use this pipeline in your cross-validation function\n",
    "cross_validation_scores(rscv, X, y, importance_attr='feature_importances_', true_importances=dgp.importances, verbose=True)\n",
    "\n",
    "best_model = rscv.best_estimator_\n",
    "# Calculate predictions for the training set and the test set\n",
    "\n",
    "print(best_model.feature_importances_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
